from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path

from jinja2 import Environment, FileSystemLoader, select_autoescape

from ..errors import CodegenError


@dataclass(frozen=True)
class BinaryOp:
    input0: str
    input1: str
    output: str
    operator: str


@dataclass(frozen=True)
class UnaryOp:
    input0: str
    output: str
    operator: str


@dataclass(frozen=True)
class MatMulOp:
    input0: str
    input1: str
    output: str
    m: int
    n: int
    k: int


@dataclass(frozen=True)
class LoweredModel:
    name: str
    input_names: tuple[str, ...]
    input_shapes: tuple[tuple[int, ...], ...]
    output_name: str
    element_count: int
    output_shape: tuple[int, ...]
    ops: tuple[BinaryOp | UnaryOp | MatMulOp, ...]


class CEmitter:
    def __init__(self, template_dir: Path) -> None:
        self._env = Environment(
            loader=FileSystemLoader(str(template_dir)),
            autoescape=select_autoescape(enabled_extensions=()),
            trim_blocks=True,
            lstrip_blocks=True,
        )

    def emit_model(self, model: LoweredModel) -> str:
        try:
            binary_template = self._env.get_template("binary_op.c.j2")
            unary_template = self._env.get_template("unary_op.c.j2")
            matmul_template = self._env.get_template("matmul_op.c.j2")
        except Exception as exc:  # pragma: no cover - template load failure
            raise CodegenError("Failed to load C template") from exc
        temp_map = self._temp_buffers(model)
        resolved_ops = [
            self._resolve_op(op, temp_map) for op in model.ops
        ]
        array_suffix = self._array_suffix(model.output_shape)
        loop_vars = self._loop_vars(model.output_shape)
        loop_indents = self._loop_indents(model.output_shape)
        inner_indent = self._inner_indent(model.output_shape)
        operator_fns = "\n\n".join(
            self._render_op(
                model,
                op,
                index,
                array_suffix=array_suffix,
                loop_vars=loop_vars,
                loop_indents=loop_indents,
                inner_indent=inner_indent,
                binary_template=binary_template,
                unary_template=unary_template,
                matmul_template=matmul_template,
            )
            for index, op in enumerate(resolved_ops)
        )
        wrapper_fn = self._emit_model_wrapper(
            model, resolved_ops, tuple(temp_map.values()), array_suffix
        )
        includes = ["#include <stddef.h>"]
        if any(
            isinstance(op, UnaryOp) and op.operator == "tanhf"
            for op in resolved_ops
        ):
            includes.append("#include <math.h>")
        rendered = "\n".join(
            (
                "// Generated by emx-onnx2c (MVP)",
                *includes,
                "",
                operator_fns.rstrip(),
                "",
                wrapper_fn,
                "",
            )
        )
        if not rendered.endswith("\n"):
            rendered += "\n"
        return rendered

    def _emit_model_wrapper(
        self,
        model: LoweredModel,
        resolved_ops: list[BinaryOp | UnaryOp | MatMulOp],
        temp_buffers: tuple[str, ...],
        array_suffix: str,
    ) -> str:
        signature = ", ".join(
            f"const float {name}{self._array_suffix(shape)}"
            for name, shape in zip(model.input_names, model.input_shapes)
        )
        lines = [
            f"void {model.name}({signature}, float {model.output_name}{array_suffix}) {{"
        ]
        for temp in temp_buffers:
            lines.append(f"    float {temp}{array_suffix};")
        for index, op in enumerate(resolved_ops):
            if isinstance(op, (BinaryOp, MatMulOp)):
                call = f"{op.input0}, {op.input1}, {op.output}"
            else:
                call = f"{op.input0}, {op.output}"
            lines.append(f"    {model.name}_op{index}({call});")
        lines.append("}")
        return "\n".join(lines)

    def _temp_buffers(self, model: LoweredModel) -> dict[str, str]:
        intermediates = [
            self._op_output(op)
            for op in model.ops
            if self._op_output(op) != model.output_name
        ]
        if not intermediates:
            return {}
        if len(intermediates) == 1:
            return {intermediates[0]: "tmp"}
        return {
            name: f"tmp{index}"
            for index, name in enumerate(intermediates)
        }

    @staticmethod
    def _resolve_op(
        op: BinaryOp | UnaryOp | MatMulOp, temp_map: dict[str, str]
    ) -> BinaryOp | UnaryOp | MatMulOp:
        if isinstance(op, BinaryOp):
            return BinaryOp(
                input0=temp_map.get(op.input0, op.input0),
                input1=temp_map.get(op.input1, op.input1),
                output=temp_map.get(op.output, op.output),
                operator=op.operator,
            )
        if isinstance(op, MatMulOp):
            return MatMulOp(
                input0=temp_map.get(op.input0, op.input0),
                input1=temp_map.get(op.input1, op.input1),
                output=temp_map.get(op.output, op.output),
                m=op.m,
                n=op.n,
                k=op.k,
            )
        return UnaryOp(
            input0=temp_map.get(op.input0, op.input0),
            output=temp_map.get(op.output, op.output),
            operator=op.operator,
        )

    @staticmethod
    def _render_op(
        model: LoweredModel,
        op: BinaryOp | UnaryOp | MatMulOp,
        index: int,
        *,
        array_suffix: str,
        loop_vars: tuple[str, ...],
        loop_indents: tuple[str, ...],
        inner_indent: str,
        binary_template,
        unary_template,
        matmul_template,
    ) -> str:
        common = {
            "model_name": model.name,
            "op_name": f"{model.name}_op{index}",
            "element_count": model.element_count,
            "array_suffix": array_suffix,
            "shape": model.output_shape,
            "loop_vars": loop_vars,
            "loop_indents": loop_indents,
            "inner_indent": inner_indent,
        }
        if isinstance(op, BinaryOp):
            return binary_template.render(
                **common,
                input0=op.input0,
                input1=op.input1,
                output=op.output,
                operator=op.operator,
            ).rstrip()
        if isinstance(op, MatMulOp):
            return matmul_template.render(
                model_name=model.name,
                op_name=f"{model.name}_op{index}",
                input0=op.input0,
                input1=op.input1,
                output=op.output,
                input0_suffix=CEmitter._array_suffix((op.m, op.k)),
                input1_suffix=CEmitter._array_suffix((op.k, op.n)),
                output_suffix=CEmitter._array_suffix((op.m, op.n)),
                m=op.m,
                n=op.n,
                k=op.k,
            ).rstrip()
        return unary_template.render(
            **common,
            input0=op.input0,
            output=op.output,
            operator=op.operator,
        ).rstrip()

    @staticmethod
    def _op_output(op: BinaryOp | UnaryOp | MatMulOp) -> str:
        return op.output

    @staticmethod
    def _array_suffix(shape: tuple[int, ...]) -> str:
        if not shape:
            raise CodegenError("Scalar outputs are not supported")
        return "".join(f"[{dim}]" for dim in shape)

    @staticmethod
    def _loop_vars(shape: tuple[int, ...]) -> tuple[str, ...]:
        if not shape:
            raise CodegenError("Scalar outputs are not supported")
        return tuple(f"i{index}" for index in range(len(shape)))

    @staticmethod
    def _loop_indents(shape: tuple[int, ...]) -> tuple[str, ...]:
        if not shape:
            raise CodegenError("Scalar outputs are not supported")
        return tuple("    " * (index + 1) for index in range(len(shape)))

    @staticmethod
    def _inner_indent(shape: tuple[int, ...]) -> str:
        if not shape:
            raise CodegenError("Scalar outputs are not supported")
        return "    " * (len(shape) + 1)
