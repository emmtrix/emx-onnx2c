/*
 * Generated by emmtrix ONNX-to-C Code Generator (emx-onnx-cgen)
 *
 * Command line: n/a
 * Model checksum (sha256): e81edfa56657356b36eeffec726961c5c46dcabae77911de9496c6dad4da16c2
 * Model name: model
 * Graph name: softmax_cross_entropy_loss_graph
 * Inputs: 2 Outputs: 2 Nodes: 1 Initializers: 0
 * IR version: 7
 * Model version: n/a
 * Domain: n/a
 * Producer: onnx2c (version: n/a)
 * Opset imports: ai.onnx=13
 * Description:
 *   n/a
 * Graph description:
 *   n/a
 * Metadata:
 *   n/a
 */

#include <stdint.h>
#include <math.h>
#include <float.h>

#ifndef idx_t
#define idx_t int32_t
#endif

static inline double ref_scalar_f64_fmax(double a, double b) {
    return fmax(a, b);
}

/*
 * Node 0:
 * OpType: SoftmaxCrossEntropyLoss
 * Name: n/a
 * Inputs: scores, labels
 * Outputs: loss, log_prob
 * Attrs:
 *   reduction: mean
 */
static inline void node0_softmaxcrossentropyloss(const float input0[restrict 2][3], const int64_t target[restrict 2], float output[restrict 1], float log_prob[restrict 2][3]) {
    const float *input_flat = (const float *)input0;
    const int64_t *target_flat = (const int64_t *)target;
    float *output_flat = (float *)output;
    float *log_prob_flat = (float *)log_prob;
    const idx_t n = 2;
    const idx_t c = 3;
    const idx_t d = 1;
    double loss_sum = 0.0;
    double weight_sum = 0.0;
    for (idx_t n_idx = 0; n_idx < n; ++n_idx) {
        for (idx_t d_idx = 0; d_idx < d; ++d_idx) {
            idx_t target_index = n_idx * d + d_idx;
            int64_t target_value = target_flat[target_index];
            idx_t class_index = (idx_t)target_value;
            idx_t base = (n_idx * c * d) + d_idx;
            double max_value = (double)input_flat[base];
            for (idx_t c_idx = 1; c_idx < c; ++c_idx) {
                double value = (double)input_flat[base + c_idx * d];
                max_value = ref_scalar_f64_fmax(max_value, value);
            }
            double sum = 0.0;
            for (idx_t c_idx = 0; c_idx < c; ++c_idx) {
                double value = (double)input_flat[base + c_idx * d] - max_value;
                sum += exp(value);
            }
            double logsum = log(sum);
            double loss_value = 0.0;
            for (idx_t c_idx = 0; c_idx < c; ++c_idx) {
                double log_prob_value = (double)input_flat[base + c_idx * d] - max_value - logsum;
                log_prob_flat[base + c_idx * d] = (float)log_prob_value;
                if (c_idx == class_index) {
                    loss_value = -log_prob_value;
                }
            }
            double sample_weight = 1.0;
            loss_sum += loss_value;
        }
    }
    output_flat[0] = loss_sum / (2 * 1);
}

_Bool model_load(const char *path) {
    (void)path;
    return 1;
}

void model(const float scores[restrict 2][3], const int64_t labels[restrict 2], float loss[restrict 1], float log_prob[restrict 2][3]) {
    node0_softmaxcrossentropyloss(scores, labels, loss, log_prob);
}
