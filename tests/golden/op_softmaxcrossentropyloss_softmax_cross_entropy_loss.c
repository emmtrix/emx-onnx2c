/*
 * Generated by emmtrix ONNX-to-C Code Generator (emx-onnx-cgen)
 *
 * Command line: n/a
 * Model checksum (sha256): e81edfa56657356b36eeffec726961c5c46dcabae77911de9496c6dad4da16c2
 * Model name: model
 * Graph name: softmax_cross_entropy_loss_graph
 * Inputs: 2 Outputs: 2 Nodes: 1 Initializers: 0
 * IR version: 7
 * Model version: n/a
 * Domain: n/a
 * Producer: onnx2c (version: n/a)
 * Opset imports: ai.onnx=13
 * Description:
 *   n/a
 * Graph description:
 *   n/a
 * Metadata:
 *   n/a
 */

#include <stddef.h>
#include <stdint.h>
#include <math.h>

/*
 * Node 0:
 * OpType: SoftmaxCrossEntropyLoss
 * Name: n/a
 * Inputs: scores, labels
 * Outputs: loss, log_prob
 * Attrs:
 *   reduction: mean
 */
static inline void node0_softmaxcrossentropyloss(const float input0[restrict 2][3], const int64_t target[restrict 2], float output[restrict 1], float log_prob[restrict 2][3]) {
    const float *input_flat = (const float *)input0;
    const int64_t *target_flat = (const int64_t *)target;
    float *output_flat = (float *)output;
    float *log_prob_flat = (float *)log_prob;
    const size_t n = 2;
    const size_t c = 3;
    const size_t d = 1;
    float loss_sum = 0.0f;
    float weight_sum = 0.0f;
    for (size_t n_idx = 0; n_idx < n; ++n_idx) {
        for (size_t d_idx = 0; d_idx < d; ++d_idx) {
            size_t target_index = n_idx * d + d_idx;
            int64_t target_value = target_flat[target_index];
            size_t class_index = (size_t)target_value;
            size_t base = (n_idx * c * d) + d_idx;
            float max_value = input_flat[base];
            for (size_t c_idx = 1; c_idx < c; ++c_idx) {
                float value = input_flat[base + c_idx * d];
                if (value > max_value) {
                    max_value = value;
                }
            }
            float sum = 0.0f;
            for (size_t c_idx = 0; c_idx < c; ++c_idx) {
                float value = input_flat[base + c_idx * d] - max_value;
                sum += expf(value);
            }
            float logsum = logf(sum);
            float loss_value = 0.0f;
            for (size_t c_idx = 0; c_idx < c; ++c_idx) {
                float log_prob_value = input_flat[base + c_idx * d] - max_value - logsum;
                log_prob_flat[base + c_idx * d] = log_prob_value;
                if (c_idx == class_index) {
                    loss_value = -log_prob_value;
                }
            }
            float sample_weight = 1.0f;
            loss_sum += loss_value;
        }
    }
    output_flat[0] = loss_sum / (2 * 1);
}

void model(const float scores[restrict 2][3], const int64_t labels[restrict 2], float loss[restrict 1], float log_prob[restrict 2][3]) {
    node0_softmaxcrossentropyloss(scores, labels, loss, log_prob);
}
